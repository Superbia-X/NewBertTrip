{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download the Flickr datasets."
      ],
      "metadata": {
        "id": "XxSotgr2J8Wy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-AmJQmepEb_e"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/KuoAiTe/BERT-Trip/main/toy_example/data/traj-Edin.csv -O traj-Edin.csv\n",
        "!wget -q https://raw.githubusercontent.com/KuoAiTe/BERT-Trip/main/toy_example/data/traj-Glas.csv -O traj-Glas.csv\n",
        "!wget -q https://raw.githubusercontent.com/KuoAiTe/BERT-Trip/main/toy_example/data/traj-Melb.csv -O traj-Melb.csv\n",
        "!wget -q https://raw.githubusercontent.com/KuoAiTe/BERT-Trip/main/toy_example/data/traj-Osak.csv -O traj-Osak.csv\n",
        "!wget -q https://raw.githubusercontent.com/KuoAiTe/BERT-Trip/main/toy_example/data/traj-Toro.csv -O traj-Toro.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the metric that computes F1 scores including the inputs."
      ],
      "metadata": {
        "id": "27wN0NHuKCY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calc_F1(traj_act, traj_rec, noloop=False):\n",
        "    '''Compute recall, precision and F1 for recommended trajectories'''\n",
        "    assert(isinstance(noloop, bool))\n",
        "    assert(len(traj_act) > 0)\n",
        "    assert(len(traj_rec) > 0)\n",
        "\n",
        "    if noloop == True:\n",
        "        intersize = len(set(traj_act) & set(traj_rec))\n",
        "    else:\n",
        "        match_tags = np.zeros(len(traj_act), dtype=bool)\n",
        "        for poi in traj_rec:\n",
        "            for j in range(len(traj_act)):\n",
        "                if match_tags[j] == False and poi == traj_act[j]:\n",
        "                    match_tags[j] = True\n",
        "                    break\n",
        "        intersize = np.nonzero(match_tags)[0].shape[0]\n",
        "\n",
        "    recall = intersize / len(traj_act)\n",
        "    precision = intersize / len(traj_rec)\n",
        "    F1 = 2 * precision * recall / (precision + recall)\n",
        "    return F1"
      ],
      "metadata": {
        "id": "NKl--jPMJHTO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the function that computes the average F1 score from a dataset."
      ],
      "metadata": {
        "id": "sgvUhVuiK4k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute(filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  # We do not use trajLen < 3 as no prediction can be made given trip origin and trip destination.\n",
        "  df = df[df['trajLen'] >= 3]\n",
        "  grouped = df.groupby('trajID')\n",
        "  f1_scores = []\n",
        "  for name, traj in grouped:\n",
        "    traj_length = len(traj)\n",
        "    expected = traj['poiID'].values\n",
        "    # Making all the prediction incorrect.\n",
        "    predicted = np.full(expected.shape, -1)\n",
        "    # Prior work concatenate the trip origin, prediction, and the trip destination to compute results.\n",
        "    # Here we followed prior work.\n",
        "    # prediction = [origin (expected[0]), predictoin, ..., ..., destination(expected[-1])]\n",
        "    predicted[0] = expected[0]\n",
        "    predicted[-1] = expected[-1]\n",
        "    f1_score = calc_F1(expected, predicted)\n",
        "    f1_scores.append(f1_score)\n",
        "  f1_scores = np.array(f1_scores)\n",
        "  return np.mean(f1_scores)"
      ],
      "metadata": {
        "id": "l-lwGUGKKQEX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we compute the results."
      ],
      "metadata": {
        "id": "XqEQ5q4HLuPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['Edin', 'Glas', 'Melb', 'Osak', 'Toro']\n",
        "\n",
        "for dataset in datasets:\n",
        "  filepath = f'traj-{dataset}.csv'\n",
        "  score = compute(filepath)\n",
        "  print(f'Dataset: {dataset} f1_score: {score} with all incorrect predictions.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RM5ndCQKNFd",
        "outputId": "02ca6ec9-14b9-4b0b-d46e-29ea8f255702"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Edin f1_score: 0.5290504885930754 with all incorrect predictions.\n",
            "Dataset: Glas f1_score: 0.5966198979591838 with all incorrect predictions.\n",
            "Dataset: Melb f1_score: 0.5176667977572955 with all incorrect predictions.\n",
            "Dataset: Osak f1_score: 0.5950354609929079 with all incorrect predictions.\n",
            "Dataset: Toro f1_score: 0.580805131849908 with all incorrect predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is why we need to exclude the trip origin and the trip destination in the computation of predictive performance."
      ],
      "metadata": {
        "id": "RXRI7efhLyT1"
      }
    }
  ]
}